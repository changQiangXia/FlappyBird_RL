"""
è®­ç»ƒå…¥å£è„šæœ¬ - é‡æ„ä¼˜åŒ–ç‰ˆ
æ”¯æŒ DQN å’Œ PPOï¼Œæ”¯æŒåƒç´ /ç‰¹å¾è¾“å…¥ï¼Œæ”¯æŒè‡ªåŠ¨æ¼”ç¤º
"""
import argparse
import os
import sys
import torch
import numpy as np
import time
from datetime import datetime
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from envs import make_env
from agents import DQNAgent, PPOAgent
from configs.dqn_config import get_dqn_config
from configs.ppo_config import get_ppo_config
from utils import Logger, save_checkpoint


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Flappy Bird RL Training')
    parser.add_argument('--algo', type=str, default='dqn', choices=['dqn', 'ppo'],
                        help='ç®—æ³•é€‰æ‹©: dqn æˆ– ppo')
    parser.add_argument('--mode', type=str, default='pixels', choices=['pixels', 'features'],
                        help='è¾“å…¥æ¨¡å¼: pixels (84x84x4) æˆ– features (8ç»´ç‰¹å¾)')
    parser.add_argument('--timesteps', type=int, default=500000,
                        help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--frame-skip', type=int, default=1,
                        help='Frame skip æ­¥æ•°ï¼ˆæ¯nå¸§å†³ç­–ä¸€æ¬¡ï¼Œé»˜è®¤1ï¼‰')
    parser.add_argument('--render-every', type=int, default=20,
                        help='æ¯Nä¸ªepisodeè‡ªåŠ¨æ¼”ç¤ºä¸€æ¬¡ï¼ˆ0è¡¨ç¤ºä¸æ¼”ç¤ºï¼Œé»˜è®¤20ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    parser.add_argument('--log-dir', type=str, default='./logs',
                        help='TensorBoard æ—¥å¿—ç›®å½•')
    parser.add_argument('--save-dir', type=str, default='./checkpoints',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®­ç»ƒè®¾å¤‡: cuda æˆ– cpu')
    parser.add_argument('--resume', type=str, default=None,
                        help='ä»checkpointæ¢å¤è®­ç»ƒï¼ˆè·¯å¾„ï¼‰')
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True


def evaluate_and_render(agent, env_config, device, n_episodes=1, fps=60, algo='dqn'):
    """è¯„ä¼°æ™ºèƒ½ä½“å¹¶æ¸²æŸ“æ¼”ç¤º"""
    render_env = make_env(
        render_mode="human",
        use_pixels=env_config['use_pixels'],
        frame_skip=1,
        use_annotated_render=True,
        seed=env_config['seed'] + 1000
    )
    
    total_rewards = []
    total_scores = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ® æ¼”ç¤ºæ¨¡å¼ï¼ˆ{n_episodes} ä¸ª episodeï¼‰")
    print(f"{'='*60}")
    
    for ep in range(n_episodes):
        obs, info = render_env.reset(seed=env_config['seed'] + 1000 + ep)
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            if algo == 'dqn':
                action = agent.select_action(obs, epsilon=0.0)
            else:
                # PPO
                with torch.no_grad():
                    actions, _, _ = agent.select_action(obs[None, ...])
                    action = actions[0]
            
            obs, reward, terminated, truncated, info = render_env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            episode_length += 1
            time.sleep(1.0 / fps)
        
        score = info.get('score', 0)
        total_rewards.append(episode_reward)
        total_scores.append(score)
        
        print(f"  Demo Ep {ep+1}: Reward={episode_reward:+.2f} | Score={score} | Steps={episode_length}")
    
    render_env.close()
    
    avg_reward = np.mean(total_rewards)
    avg_score = np.mean(total_scores)
    
    print(f"  å¹³å‡: Reward={avg_reward:+.2f} | Score={avg_score:.1f}")
    print(f"{'='*60}\n")
    
    return avg_reward, avg_score


def train_dqn(args):
    """DQN è®­ç»ƒæµç¨‹ - ä¼˜åŒ–ç‰ˆ"""
    print("=" * 60)
    print("ğŸš€ DQN è®­ç»ƒå¼€å§‹")
    print(f"   è¾“å…¥æ¨¡å¼: {args.mode}")
    print(f"   Frame Skip: {args.frame_skip}")
    print(f"   æ¼”ç¤ºé—´éš”: æ¯ {args.render_every} episode")
    print(f"   è®¾å¤‡: {args.device}")
    print("=" * 60)
    
    use_pixels = (args.mode == 'pixels')
    config = get_dqn_config(use_pixels=use_pixels)
    config.total_timesteps = args.timesteps
    config.device = args.device
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    env = make_env(
        render_mode=None,
        use_pixels=use_pixels,
        frame_skip=args.frame_skip,
        use_annotated_render=False,
        seed=args.seed
    )
    
    obs_shape = env.observation_space.shape
    n_actions = env.action_space.n
    print(f"è§‚å¯Ÿç©ºé—´: {obs_shape}, åŠ¨ä½œç©ºé—´: {n_actions}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(obs_shape, n_actions, config, device=args.device)
    print(f"ç½‘ç»œå‚æ•°: {sum(p.numel() for p in agent.parameters()):,}")
    
    # åŠ è½½checkpoint
    if args.resume and os.path.exists(args.resume):
        print(f"\nğŸ“‚ åŠ è½½ checkpoint: {args.resume}")
        agent.load(args.resume, reset_epsilon=0.3)
    
    # åˆ›å»ºæ—¥å¿—å™¨
    logger = Logger(args.log_dir, f"DQN_{args.mode}")
    
    # è®­ç»ƒå¾ªç¯
    obs, info = env.reset(seed=args.seed)
    episode = 0
    episode_reward = 0
    episode_length = 0
    best_reward = -float('inf')
    best_score = 0
    total_steps = 0
    
    demo_env_config = {'use_pixels': use_pixels, 'seed': args.seed}
    
    pbar = tqdm(total=args.timesteps, desc="Training DQN", ncols=100)
    
    while total_steps < args.timesteps:
        # é€‰æ‹©åŠ¨ä½œ
        action = agent.select_action(obs, epsilon=agent.epsilon)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        # å­˜å‚¨è½¬ç§»
        current_score = info.get('score', 0)
        agent.store_transition(obs, action, reward, next_obs, done, score=current_score)
        
        episode_reward += reward
        episode_length += 1
        total_steps += 1
        obs = next_obs
        
        # å­¦ä¹ 
        if total_steps >= config.learning_starts and total_steps % config.train_freq == 0:
            loss_dict = agent.learn(config.batch_size if use_pixels else config.batch_size_feature)
            if total_steps % 1000 == 0:
                logger.log_step(total_steps, loss_dict)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if total_steps % config.target_update_freq == 0 and total_steps > config.learning_starts:
            agent.update_target_network()
        
        # Episode ç»“æŸ
        if done:
            episode += 1
            score = info.get('score', 0)
            
            # è®°å½•æŒ‡æ ‡
            logger.log_episode(episode, {
                'reward': episode_reward,
                'length': episode_length,
                'score': score,
                'epsilon': agent.epsilon
            })
            
            # æ›´æ–°æœ€ä½³
            if episode_reward > best_reward:
                best_reward = episode_reward
            if score > best_score:
                best_score = score
                save_checkpoint(
                    agent.policy_net.state_dict(),
                    agent.optimizer.state_dict(),
                    episode, total_steps,
                    os.path.join(args.save_dir, f"DQN_{args.mode}"),
                    is_best=True
                )
            
            # å®šæœŸä¿å­˜
            if episode % config.save_interval == 0:
                save_checkpoint(
                    agent.policy_net.state_dict(),
                    agent.optimizer.state_dict(),
                    episode, total_steps,
                    os.path.join(args.save_dir, f"DQN_{args.mode}")
                )
            
            # æ‰“å°è¿›åº¦
            if episode % config.log_interval == 0:
                tqdm.write(f"Ep {episode:5d} | Step {total_steps:6d} | "
                          f"Reward: {episode_reward:+7.2f} | Score: {score:2d} | "
                          f"Best: {best_score:2d} | Îµ: {agent.epsilon:.3f}")
            
            # è‡ªåŠ¨æ¼”ç¤º
            if args.render_every > 0 and episode % args.render_every == 0:
                demo_reward, demo_score = evaluate_and_render(
                    agent, demo_env_config, args.device, n_episodes=1, fps=60, algo='dqn'
                )
                logger.log_scalar('demo/reward', demo_reward, episode)
                logger.log_scalar('demo/score', demo_score, episode)
            
            # é‡ç½®
            obs, info = env.reset(seed=args.seed + episode)
            episode_reward = 0
            episode_length = 0
        
        pbar.update(1)
    
    pbar.close()
    env.close()
    logger.close()
    
    print(f"\n{'='*60}")
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ€» episode: {episode}")
    print(f"   æ€»æ­¥æ•°: {total_steps}")
    print(f"   æœ€ä½³å¥–åŠ±: {best_reward:+.2f}")
    print(f"   æœ€é«˜åˆ†æ•°: {best_score}")
    print(f"{'='*60}")


def train_ppo(args):
    """PPO è®­ç»ƒæµç¨‹ - ä¼˜åŒ–ç‰ˆ"""
    print("=" * 60)
    print("ğŸš€ PPO è®­ç»ƒå¼€å§‹")
    print(f"   è¾“å…¥æ¨¡å¼: {args.mode}")
    print(f"   Frame Skip: {args.frame_skip}")
    print(f"   æ¼”ç¤ºé—´éš”: æ¯ {args.render_every} episode")
    print(f"   è®¾å¤‡: {args.device}")
    print("=" * 60)
    
    use_pixels = (args.mode == 'pixels')
    config = get_ppo_config(use_pixels=use_pixels)
    config.total_timesteps = args.timesteps
    config.device = args.device
    
    n_envs = config.n_envs
    
    # åˆ›å»ºå¤šä¸ªç¯å¢ƒ
    envs = [
        make_env(
            render_mode=None,
            use_pixels=use_pixels,
            frame_skip=args.frame_skip,
            use_annotated_render=False,
            seed=args.seed + i
        ) for i in range(n_envs)
    ]
    
    obs_shape = envs[0].observation_space.shape
    n_actions = envs[0].action_space.n
    print(f"è§‚å¯Ÿç©ºé—´: {obs_shape}, åŠ¨ä½œç©ºé—´: {n_actions}")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {n_envs}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = PPOAgent(obs_shape, n_actions, config, device=args.device)
    print(f"ç½‘ç»œå‚æ•°: {sum(p.numel() for p in agent.parameters()):,}")
    
    # åŠ è½½checkpoint
    if args.resume and os.path.exists(args.resume):
        print(f"\nğŸ“‚ åŠ è½½ checkpoint: {args.resume}")
        agent.load(args.resume)
    
    # åˆ›å»ºæ—¥å¿—å™¨
    logger = Logger(args.log_dir, f"PPO_{args.mode}")
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    obs_list = [env.reset(seed=args.seed + i)[0] for i, env in enumerate(envs)]
    obs_batch = np.array(obs_list)
    
    episode_rewards = [0.0] * n_envs
    episode_lengths = [0] * n_envs
    total_episodes = 0
    best_reward = -float('inf')
    best_score = 0
    total_steps = 0
    
    demo_env_config = {'use_pixels': use_pixels, 'seed': args.seed}
    
    n_steps = config.n_steps if use_pixels else config.n_steps_feature
    total_updates = args.timesteps // (n_steps * n_envs)
    
    pbar = tqdm(total=total_updates, desc="Training PPO", ncols=100)
    
    for update in range(1, total_updates + 1):
        # æ”¶é›†ç»éªŒ
        for step in range(n_steps):
            actions, log_probs, values = agent.select_action(obs_batch)
            
            next_obs_list = []
            rewards = np.zeros(n_envs, dtype=np.float32)
            dones = np.zeros(n_envs, dtype=np.float32)
            
            for i, env in enumerate(envs):
                next_obs, reward, terminated, truncated, info = env.step(actions[i])
                done = terminated or truncated
                
                next_obs_list.append(next_obs)
                rewards[i] = reward
                dones[i] = float(done)
                
                episode_rewards[i] += reward
                episode_lengths[i] += 1
                
                if done:
                    total_episodes += 1
                    score = info.get('score', 0)
                    
                    logger.log_episode(total_episodes, {
                        'reward': episode_rewards[i],
                        'length': episode_lengths[i],
                        'score': score
                    })
                    
                    if episode_rewards[i] > best_reward:
                        best_reward = episode_rewards[i]
                    if score > best_score:
                        best_score = score
                    
                    if total_episodes % config.log_interval == 0:
                        tqdm.write(f"Ep {total_episodes:5d} | Update {update:4d} | "
                                  f"Reward: {episode_rewards[i]:+7.2f} | Score: {score:2d} | "
                                  f"Best: {best_score:2d}")
                    
                    # è‡ªåŠ¨æ¼”ç¤º
                    if args.render_every > 0 and total_episodes % args.render_every == 0:
                        demo_reward, demo_score = evaluate_and_render(
                            agent, demo_env_config, args.device, n_episodes=1, fps=60, algo='ppo'
                        )
                        logger.log_scalar('demo/reward', demo_reward, total_episodes)
                        logger.log_scalar('demo/score', demo_score, total_episodes)
                    
                    obs_reset, _ = env.reset(seed=args.seed + total_episodes + i)
                    next_obs_list[i] = obs_reset
                    episode_rewards[i] = 0.0
                    episode_lengths[i] = 0
            
            agent.store_transition(obs_batch, actions, log_probs, rewards, values, dones)
            obs_batch = np.array(next_obs_list)
            total_steps += n_envs
        
        # è®¡ç®—ä¼˜åŠ¿å¹¶æ›´æ–°
        agent.compute_advantages(obs_batch)
        loss_dict = agent.learn()
        logger.log_step(update, loss_dict)
        
        # å®šæœŸä¿å­˜
        if update % config.save_interval == 0:
            save_checkpoint(
                agent.network.state_dict(),
                agent.optimizer.state_dict(),
                update, total_steps,
                os.path.join(args.save_dir, f"PPO_{args.mode}")
            )
        
        pbar.update(1)
    
    pbar.close()
    
    for env in envs:
        env.close()
    logger.close()
    
    print(f"\n{'='*60}")
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ€»æ›´æ–°: {total_updates}")
    print(f"   æ€» episode: {total_episodes}")
    print(f"   æœ€ä½³å¥–åŠ±: {best_reward:+.2f}")
    print(f"   æœ€é«˜åˆ†æ•°: {best_score}")
    print(f"{'='*60}")


def main():
    args = parse_args()
    set_seed(args.seed)
    
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° CPU")
        args.device = "cpu"
    
    print(f"ä½¿ç”¨è®¾å¤‡: {args.device}")
    if args.device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(args.save_dir, exist_ok=True)
    
    if args.algo == 'dqn':
        train_dqn(args)
    else:
        train_ppo(args)


if __name__ == "__main__":
    main()
