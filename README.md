# Flappy Bird RL - Deep Reinforcement Learning Framework

**[English](#english-version) | [ä¸­æ–‡](#ä¸­æ–‡ç‰ˆæœ¬)**

Click above to switch language / ç‚¹å‡»ä¸Šæ–¹åˆ‡æ¢è¯­è¨€

---

# ä¸­æ–‡ç‰ˆæœ¬

<div id="ä¸­æ–‡ç‰ˆæœ¬"></div>

[è·³è½¬åˆ°è‹±æ–‡ç‰ˆæœ¬](#english-version)

æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œä½¿ç”¨ DQN å’Œ PPO ç®—æ³•è®­ç»ƒ Flappy Bird æ™ºèƒ½ä½“ã€‚

<p align="center">
  <img src="https://raw.githubusercontent.com/changQiangXia/FlappyBird_RL/main/docs/training_demo.png" alt="è®­ç»ƒç•Œé¢" width="500"/>
  <br>
  <em>å›¾1ï¼šè®­ç»ƒæ—¶çš„ Pygame å¯è§†åŒ–ç•Œé¢ï¼Œæ˜¾ç¤ºåˆ†æ•°ã€å¥–åŠ±ã€æ­¥æ•°å’ŒåŠ¨ä½œä¿¡æ¯</em>
</p>

---

## ğŸ“‹ ç›®å½•

1. [è®­ç»ƒæˆæœå±•ç¤º](#è®­ç»ƒæˆæœå±•ç¤º)
2. [ç®—æ³•å®ç°æ€è·¯](#ç®—æ³•å®ç°æ€è·¯)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è¯¦ç»†è®­ç»ƒæŒ‡å—](#è¯¦ç»†è®­ç»ƒæŒ‡å—)
5. [Checkpointç®¡ç†](#checkpointç®¡ç†)
6. [æ¨¡å‹æµ‹è¯•ä¸æ¼”ç¤º](#æ¨¡å‹æµ‹è¯•ä¸æ¼”ç¤º)
7. [è®­ç»ƒæŠ€å·§ä¸è°ƒå‚](#è®­ç»ƒæŠ€å·§ä¸è°ƒå‚)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ† è®­ç»ƒæˆæœå±•ç¤º

### PPO ç®—æ³• 100ä¸‡æ­¥è®­ç»ƒç»“æœ

<p align="center">
  <img src="https://raw.githubusercontent.com/changQiangXia/FlappyBird_RL/main/docs/ppo_1m_result.png" alt="PPOè®­ç»ƒç»“æœ" width="600"/>
  <br>
  <em>å›¾2ï¼šPPOç®—æ³•è®­ç»ƒ100ä¸‡æ­¥åçš„ç»“æœï¼Œæœ€é«˜åˆ†æ•°è¾¾åˆ°240+ï¼Œå±•ç°äº†ä¼˜ç§€çš„æŒç»­é£è¡Œèƒ½åŠ›</em>
</p>

**å…³é”®æ•°æ®ï¼š**
- æœ€é«˜åˆ†æ•°ï¼š**240+** æ ¹ç®¡å­
- ç¨³å®šåˆ†æ•°ï¼š50-100 æ ¹ç®¡å­
- è®­ç»ƒæ—¶é—´ï¼šçº¦2-3å°æ—¶
- ç®—æ³•ï¼šPPO + Featuresæ¨¡å¼

---

## ğŸ§  ç®—æ³•å®ç°æ€è·¯

### DQN (Deep Q-Network)

**æ ¸å¿ƒæ¶æ„ï¼š**
è¾“å…¥ -> CNNç‰¹å¾æå– -> Duelingç»“æ„ -> Qå€¼è¾“å‡º

**å…³é”®æŠ€æœ¯ç‚¹ï¼š**

| æŠ€æœ¯ | è¯´æ˜ | ä½œç”¨ |
|-----|------|------|
| Double DQN | ç”¨Policyç½‘ç»œé€‰åŠ¨ä½œï¼ŒTargetç½‘ç»œç®—Qå€¼ | è§£å†³Qå€¼è¿‡ä¼°è®¡é—®é¢˜ |
| Dueling DQN | åˆ†ç¦»Valueæµå’ŒAdvantageæµ | æ›´ç¨³å®šåœ°å­¦ä¹ å“ªäº›çŠ¶æ€æœ‰ä»·å€¼ |
| ç»éªŒå›æ”¾ | å­˜å‚¨è½¬ç§»æ ·æœ¬ï¼Œéšæœºé‡‡æ ·è®­ç»ƒ | æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§ï¼Œæé«˜æ•°æ®æ•ˆç‡ |
| ç›®æ ‡ç½‘ç»œ | å®šæœŸå¤åˆ¶Policyç½‘ç»œå‚æ•° | ç¨³å®šå­¦ä¹ ç›®æ ‡ï¼Œé¿å…éœ‡è¡ |
| Epsilon-Greedy | ä»¥Îµæ¦‚ç‡éšæœºæ¢ç´¢ | å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ |
| æ··åˆç²¾åº¦ | FP16è®­ç»ƒï¼ŒFP32æ¢¯åº¦æ›´æ–° | åŠ é€Ÿè®­ç»ƒï¼ŒèŠ‚çœæ˜¾å­˜ |

**ç½‘ç»œç»“æ„ï¼ˆPixelsæ¨¡å¼ï¼‰ï¼š**
```
Conv2d(4->32, 8x8, stride=4) -> ReLU
Conv2d(32->64, 4x4, stride=2) -> ReLU  
Conv2d(64->64, 3x3, stride=1) -> ReLU
Flatten -> Linear(3136->512) -> ReLU
Valueæµ: Linear(512->1)
Advantageæµ: Linear(512->n_actions)
Q = Value + (Advantage - mean(Advantage))
```

**è¶…å‚æ•°ï¼š**
- å­¦ä¹ ç‡ï¼š1e-4
- æŠ˜æ‰£å› å­gammaï¼š0.99
- Epsilonè¡°å‡ï¼š1.0 -> 0.05ï¼ˆ25,000æ­¥ï¼‰
- ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼šæ¯1000æ­¥
- Batch sizeï¼š64
- Bufferå¤§å°ï¼š50,000

---

### PPO (Proximal Policy Optimization)

**æ ¸å¿ƒæ¶æ„ï¼š**
è¾“å…¥ -> å…±äº«CNN -> Actor(ç­–ç•¥) + Critic(ä»·å€¼)

**å…³é”®æŠ€æœ¯ç‚¹ï¼š**

| æŠ€æœ¯ | è¯´æ˜ | ä½œç”¨ |
|-----|------|------|
| Actor-Critic | Actorè¾“å‡ºåŠ¨ä½œæ¦‚ç‡ï¼ŒCriticè¯„ä¼°çŠ¶æ€ä»·å€¼ | ç»“åˆç­–ç•¥æ¢¯åº¦å’Œå€¼å‡½æ•°è¿‘ä¼¼ |
| GAE | å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ | å¹³è¡¡åå·®ä¸æ–¹å·®ï¼Œç¨³å®šä¼˜åŠ¿è®¡ç®— |
| PPO-Clip | é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ | é˜²æ­¢ç­–ç•¥çªå˜ï¼Œè®­ç»ƒæ›´ç¨³å®š |
| å¤šç¯å¢ƒå¹¶è¡Œ | åŒæ—¶è¿è¡Œ4-8ä¸ªç¯å¢ƒ | æ ·æœ¬æ”¶é›†æ›´å¿«ï¼Œæ•°æ®æ›´å¤šæ · |
| æ­£äº¤åˆå§‹åŒ– | ç½‘ç»œæƒé‡æ­£äº¤åˆå§‹åŒ– | æ”¹å–„åˆå§‹æ¢¯åº¦æµ |
| ç†µæ­£åˆ™ | æœ€å¤§åŒ–ç­–ç•¥ç†µ | é¼“åŠ±æ¢ç´¢ï¼Œé¿å…è¿‡æ—©æ”¶æ•› |

**ç½‘ç»œç»“æ„ï¼š**
```
å…±äº«ç‰¹å¾æå–å™¨ï¼ˆåŒDQN CNNï¼‰
|-- Actor: Linear(512->2) -> Softmax -> åŠ¨ä½œæ¦‚ç‡
|-- Critic: Linear(512->1) -> çŠ¶æ€ä»·å€¼
```

**PPO-Clipç›®æ ‡å‡½æ•°ï¼š**
```
L^CLIP(Î¸) = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)]
å…¶ä¸­ r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)ï¼ŒÎµ=0.2
```

**è¶…å‚æ•°ï¼š**
- å­¦ä¹ ç‡ï¼š2.5e-4
- æŠ˜æ‰£å› å­gammaï¼š0.99
- GAE lambdaï¼š0.95
- å¹¶è¡Œç¯å¢ƒæ•°ï¼š4ï¼ˆpixelsï¼‰/ 8ï¼ˆfeaturesï¼‰
- æ¯ç¯å¢ƒæ­¥æ•°ï¼š512ï¼ˆpixelsï¼‰/ 1024ï¼ˆfeaturesï¼‰
- è®­ç»ƒè½®æ•°ï¼š4è½®
- ClipèŒƒå›´ï¼š0.2
- ç†µç³»æ•°ï¼š0.01

---

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n flappy_rl python=3.10
conda activate flappy_rl

# å®‰è£…ä¾èµ–
pip install torch torchvision gymnasium flappy-bird-gymnasium opencv-python tqdm tensorboard
```

### 3åˆ†é’Ÿå¿«é€Ÿè®­ç»ƒ

```bash
# æœ€å¿«çš„è®­ç»ƒæ–¹å¼ï¼ˆfeaturesæ¨¡å¼ï¼‰
python train.py --algo ppo --mode features --timesteps 50000 --render-every 10
```

é¢„æœŸæ•ˆæœï¼š5-10åˆ†é’Ÿåï¼Œæ™ºèƒ½ä½“èƒ½é€šè¿‡5-10æ ¹ç®¡å­ã€‚

---

## è¯¦ç»†è®­ç»ƒæŒ‡å—

### è®­ç»ƒå‘½ä»¤ç»“æ„

```bash
python train.py \
    --algo {dqn|ppo} \              # é€‰æ‹©ç®—æ³•
    --mode {pixels|features} \      # é€‰æ‹©è¾“å…¥æ¨¡å¼
    --timesteps N \                 # æ€»è®­ç»ƒæ­¥æ•°
    --render-every M \              # æ¯Må±€æ¼”ç¤ºä¸€æ¬¡
    --frame-skip K \                # æ¯Kå¸§å†³ç­–ä¸€æ¬¡
    --seed S \                      # éšæœºç§å­
    --device {cuda|cpu}             # è®­ç»ƒè®¾å¤‡
```

### è®­ç»ƒæ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | è¾“å…¥ç»´åº¦ | é€Ÿåº¦ | æ”¶æ•›éš¾åº¦ | æ¨èåœºæ™¯ |
|-----|---------|------|---------|---------|
| **Features** | 8ç»´å‘é‡ | ~500 it/s | å®¹æ˜“ | å¿«é€ŸéªŒè¯ã€ç”Ÿäº§è®­ç»ƒ |
| **Pixels** | 84x84x4å›¾åƒ | ~100 it/s | è¾ƒéš¾ | ç ”ç©¶ã€ç«¯åˆ°ç«¯å­¦ä¹  |

**Featuresæ¨¡å¼è¾“å…¥ï¼š**
- å°é¸ŸYåæ ‡
- å°é¸Ÿå‚ç›´é€Ÿåº¦
- ä¸‹æ ¹ç®¡é“Xè·ç¦»
- ä¸‹æ ¹ä¸Šç®¡é“Yåæ ‡
- ä¸‹æ ¹ä¸‹ç®¡é“Yåæ ‡
- å†ä¸‹æ ¹ç®¡é“Xè·ç¦»
- å†ä¸‹æ ¹ä¸Šç®¡é“Yåæ ‡
- å†ä¸‹æ ¹ä¸‹ç®¡é“Yåæ ‡

### æ¨èè®­ç»ƒæ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šå¿«é€ŸéªŒè¯ï¼ˆ5åˆ†é’Ÿï¼‰**
```bash
python train.py --algo ppo --mode features --timesteps 50000 --render-every 5
```
é¢„æœŸåˆ†æ•°ï¼š5-10åˆ†

**æ–¹æ¡ˆ2ï¼šç¨³å®šè®­ç»ƒï¼ˆ30åˆ†é’Ÿï¼‰**
```bash
python train.py --algo ppo --mode features --timesteps 200000 --render-every 20
```
é¢„æœŸåˆ†æ•°ï¼š30-50åˆ†

**æ–¹æ¡ˆ3ï¼šå†²å‡»é«˜åˆ†ï¼ˆ2-3å°æ—¶ï¼‰**
```bash
python train.py --algo ppo --mode features --timesteps 1000000 --render-every 100
```
é¢„æœŸåˆ†æ•°ï¼š100+åˆ†ï¼ˆå®é™…å¯è¾¾240+ï¼‰

**æ–¹æ¡ˆ4ï¼šåƒç´ æ¨¡å¼è®­ç»ƒï¼ˆè¾ƒæ…¢ï¼‰**
```bash
# DQN Pixels
python train.py --algo dqn --mode pixels --timesteps 200000 --render-every 20 --frame-skip 1

# PPO Pixels
python train.py --algo ppo --mode pixels --timesteps 200000 --render-every 20
```

### ä»Checkpointæ¢å¤è®­ç»ƒ

```bash
# ç»§ç»­è®­ç»ƒ
python train.py --algo ppo --mode features --timesteps 500000 \
    --resume checkpoints/PPO_features/model_latest.pt
```

---

## Checkpointç®¡ç†

### ä¿å­˜ä½ç½®

Checkpointè‡ªåŠ¨ä¿å­˜åœ¨ `./checkpoints/` ç›®å½•ä¸‹ï¼š

```
checkpoints/
â”œâ”€â”€ DQN_pixels/
â”‚   â”œâ”€â”€ model_best.pt        # æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰åˆ†æ•°ï¼‰
â”‚   â”œâ”€â”€ model_latest.pt      # æœ€æ–°æ¨¡å‹
â”‚   â”œâ”€â”€ model_ep100.pt       # ç¬¬100å±€æ¨¡å‹
â”‚   â””â”€â”€ ...
â”œâ”€â”€ DQN_features/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ PPO_pixels/
â”‚   â””â”€â”€ ...
â””â”€â”€ PPO_features/
    â”œâ”€â”€ model_latest.pt
    â”œâ”€â”€ model_ep20.pt
    â””â”€â”€ ...
```

### ä¸åŒç®—æ³•çš„ä¿å­˜ç­–ç•¥

| ç®—æ³• | ä¿å­˜å†…å®¹ | å‘½åè§„åˆ™ |
|-----|---------|---------|
| **DQN** | policy_net, target_net, optimizer, epsilon | model_best.pt, model_ep{N}.pt |
| **PPO** | network (Actor+Critic), optimizer | model_latest.pt, model_ep{N}.pt |

**æ³¨æ„ï¼š**
- DQNæœ‰model_best.ptï¼ˆä¿å­˜å†å²æœ€é«˜åˆ†æ¨¡å‹ï¼‰
- PPOåªæœ‰model_latest.ptå’ŒæŒ‰episodeä¿å­˜çš„æ¨¡å‹

---

## æ¨¡å‹æµ‹è¯•ä¸æ¼”ç¤º

### æ’­æ”¾è®­ç»ƒå¥½çš„æ¨¡å‹

```bash
# åŸºæœ¬ç”¨æ³•
python play.py --checkpoint <è·¯å¾„> --algo <ç®—æ³•> --mode <æ¨¡å¼>

# æ’­æ”¾PPO featuresæœ€æ–°æ¨¡å‹
python play.py \
    --checkpoint checkpoints/PPO_features/model_latest.pt \
    --algo ppo \
    --mode features
```

### å¸¸ç”¨æ’­æ”¾å‘½ä»¤

**æ’­æ”¾PPO featuresæœ€æ–°æ¨¡å‹ï¼Œ5å±€ï¼š**
```bash
python play.py --checkpoint checkpoints/PPO_features/model_latest.pt --algo ppo --mode features
```

**æ’­æ”¾DQN featuresæœ€ä½³æ¨¡å‹ï¼Œ10å±€ï¼Œæ…¢é€Ÿï¼š**
```bash
python play.py --checkpoint checkpoints/DQN_features/model_best.pt --algo dqn --mode features --episodes 10 --fps 30
```

**æ’­æ”¾DQN pixelsæ¨¡å‹ï¼š**
```bash
python play.py --checkpoint checkpoints/DQN_pixels/model_best.pt --algo dqn --mode pixels
```

### æ¼”ç¤ºæ—¶çš„å¯è§†åŒ–ä¿¡æ¯

æ’­æ”¾æ—¶ä¼šæ˜¾ç¤ºï¼š
- **SCORE**: å½“å‰åˆ†æ•°
- **REWARD**: æœ¬å±€ç´¯è®¡å¥–åŠ±
- **STEPS**: å·²é£è¡Œæ­¥æ•°
- **ACTION**: å½“å‰åŠ¨ä½œï¼ˆFLAP/NONEï¼‰
- **DIST**: åˆ°ç®¡é“ä¸­å¿ƒçš„è·ç¦»

---

## è®­ç»ƒæŠ€å·§ä¸è°ƒå‚

### å¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆå…³é”®ï¼ï¼‰

å½“å‰å¥–åŠ±æ„æˆï¼ˆenvs/wrappers.pyï¼‰ï¼š

```
+25.0   # é€šè¿‡ä¸€æ ¹ç®¡å­ï¼ˆæ ¸å¿ƒå¥–åŠ±ï¼‰
+2.0    # ä½ç½®å¥–åŠ±ï¼ˆé«˜æ–¯åˆ†å¸ƒï¼Œè¶Šæ¥è¿‘ç®¡é“ä¸­å¿ƒè¶Šé«˜ï¼‰
+0.5    # åœ¨ç®¡é“é—´éš™èŒƒå›´å†…é¢å¤–å¥–åŠ±
+0.05   # æ¯å¸§ç”Ÿå­˜å¥–åŠ±
+0.1    # è¿›æ­¥å¥–åŠ±ï¼ˆç®¡é“åœ¨é è¿‘ï¼‰
-0.2    # å¤ªé è¿‘å¤©èŠ±æ¿/åœ°æ¿æƒ©ç½š
-5.0    # æ­»äº¡æƒ©ç½š
```

**è°ƒå‚å»ºè®®ï¼š**
- å¦‚æœæ™ºèƒ½ä½“"è‹Ÿæ´»"ä¸è¿‡ç®¡ï¼šæé«˜PASS_REWARDï¼ˆå¦‚+50ï¼‰
- å¦‚æœæ™ºèƒ½ä½“æ’å¤©èŠ±æ¿/åœ°æ¿ï¼šæé«˜ä½ç½®å¥–åŠ±ï¼ŒåŠ å¼ºé«˜åº¦æƒ©ç½š
- å¦‚æœæ™ºèƒ½ä½“é€šè¿‡ä½†åˆ†æ•°ä¸é«˜ï¼šæ·»åŠ é€Ÿåº¦å¥–åŠ±ï¼Œé¼“åŠ±å¿«é€Ÿé€šè¿‡

### TensorBoardç›‘æ§

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir logs/

# æµè§ˆå™¨æ‰“å¼€ http://localhost:6006
```

**å…³é”®æŒ‡æ ‡ï¼š**
- episode/score: æ¯å±€åˆ†æ•°ï¼ˆæœ€é‡è¦ï¼‰
- episode/reward: æ¯å±€å¥–åŠ±
- step/loss: è®­ç»ƒæŸå¤±
- demo/score: æ¼”ç¤ºåˆ†æ•°

---

## æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šCUDA OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
python train.py --algo dqn --mode features --timesteps 100000 --device cpu
```

### é—®é¢˜2ï¼šModuleNotFoundError

```bash
conda activate flappy_rl
pip install gymnasium flappy-bird-gymnasium
```

### é—®é¢˜3ï¼šåˆ†æ•°å¡åœ¨1-2åˆ†ä¸æå‡

**è§£å†³æ–¹æ¡ˆï¼š**
1. å°è¯•PPOç®—æ³•ï¼ˆé€šå¸¸æ¯”DQNæ›´æ˜“æ”¶æ•›ï¼‰
2. ä½¿ç”¨featuresæ¨¡å¼
3. å»¶é•¿è®­ç»ƒæ—¶é—´ï¼ˆè‡³å°‘5ä¸‡æ­¥ï¼‰

---

**ç¥è®­ç»ƒæ„‰å¿«ï¼æœŸå¾…ä½ çš„æ™ºèƒ½ä½“çªç ´100åˆ†ï¼** ğŸš€ğŸ¦

---

<div id="english-version"></div>

[Back to Chinese Version](#ä¸­æ–‡ç‰ˆæœ¬)

---

# English Version

Deep Reinforcement Learning training framework using DQN and PPO algorithms to train Flappy Bird agents.

<p align="center">
  <img src="https://raw.githubusercontent.com/changQiangXia/FlappyBird_RL/main/docs/training_demo.png" alt="Training Interface" width="500"/>
  <br>
  <em>Figure 1: Pygame visualization interface during training, showing score, reward, steps and action information</em>
</p>

---

## Table of Contents

1. [Training Results](#training-results)
2. [Algorithm Implementation](#algorithm-implementation)
3. [Quick Start](#quick-start)
4. [Detailed Training Guide](#detailed-training-guide)
5. [Checkpoint Management](#checkpoint-management)
6. [Model Testing & Demo](#model-testing--demo)
7. [Training Tips](#training-tips)
8. [Troubleshooting](#troubleshooting)

---

## Training Results

### PPO Algorithm - 1 Million Steps

<p align="center">
  <img src="https://raw.githubusercontent.com/changQiangXia/FlappyBird_RL/main/docs/ppo_1m_result.png" alt="PPO Training Result" width="600"/>
  <br>
  <em>Figure 2: PPO algorithm results after 1 million training steps, achieving highest score of 240+ pipes, demonstrating excellent sustained flight capability</em>
</p>

**Key Metrics:**
- Highest Score: **240+** pipes
- Stable Score: 50-100 pipes
- Training Time: ~2-3 hours
- Algorithm: PPO + Features mode

---

## Algorithm Implementation

### DQN (Deep Q-Network)

**Core Architecture:**
Input -> CNN Feature Extraction -> Dueling Structure -> Q-value Output

**Key Techniques:**

| Technique | Description | Purpose |
|-----------|-------------|---------|
| Double DQN | Use Policy network to select actions, Target network to compute Q-values | Solve Q-value overestimation |
| Dueling DQN | Separate Value stream and Advantage stream | More stable state value learning |
| Experience Replay | Store transitions, sample randomly for training | Break sample correlation, improve data efficiency |
| Target Network | Periodically copy Policy network parameters | Stabilize learning target |
| Epsilon-Greedy | Random exploration with probability Îµ | Balance exploration and exploitation |
| Mixed Precision | FP16 training, FP32 gradient update | Speed up training, save memory |

**Network Structure (Pixels Mode):**
```
Conv2d(4->32, 8x8, stride=4) -> ReLU
Conv2d(32->64, 4x4, stride=2) -> ReLU  
Conv2d(64->64, 3x3, stride=1) -> ReLU
Flatten -> Linear(3136->512) -> ReLU
Value stream: Linear(512->1)
Advantage stream: Linear(512->n_actions)
Q = Value + (Advantage - mean(Advantage))
```

**Hyperparameters:**
- Learning rate: 1e-4
- Discount factor gamma: 0.99
- Epsilon decay: 1.0 -> 0.05 (25,000 steps)
- Target network update: every 1000 steps
- Batch size: 64
- Buffer size: 50,000

---

### PPO (Proximal Policy Optimization)

**Core Architecture:**
Input -> Shared CNN -> Actor (Policy) + Critic (Value)

**Key Techniques:**

| Technique | Description | Purpose |
|-----------|-------------|---------|
| Actor-Critic | Actor outputs action probabilities, Critic evaluates state value | Combine policy gradient and value function approximation |
| GAE | Generalized Advantage Estimation | Balance bias and variance |
| PPO-Clip | Limit policy update magnitude | Prevent policy collapse |
| Parallel Environments | Run 4-8 environments simultaneously | Faster sample collection |
| Orthogonal Initialization | Orthogonal weight initialization | Improve initial gradient flow |
| Entropy Regularization | Maximize policy entropy | Encourage exploration |

**Network Structure:**
```
Shared Feature Extractor (same as DQN CNN)
|-- Actor: Linear(512->2) -> Softmax -> Action probabilities
|-- Critic: Linear(512->1) -> State value
```

**PPO-Clip Objective:**
```
L^CLIP(Î¸) = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)]
where r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s), Îµ=0.2
```

**Hyperparameters:**
- Learning rate: 2.5e-4
- Discount factor gamma: 0.99
- GAE lambda: 0.95
- Parallel environments: 4 (pixels) / 8 (features)
- Steps per environment: 512 (pixels) / 1024 (features)
- Training epochs: 4
- Clip range: 0.2
- Entropy coefficient: 0.01

---

## Quick Start

### Environment Setup

```bash
# Create conda environment
conda create -n flappy_rl python=3.10
conda activate flappy_rl

# Install dependencies
pip install torch torchvision gymnasium flappy-bird-gymnasium opencv-python tqdm tensorboard
```

### 3-Minute Quick Training

```bash
# Fastest training method (features mode)
python train.py --algo ppo --mode features --timesteps 50000 --render-every 10
```

Expected results: After 5-10 minutes, the agent can pass 5-10 pipes.

---

## Detailed Training Guide

### Training Command Structure

```bash
python train.py \
    --algo {dqn|ppo} \              # Select algorithm
    --mode {pixels|features} \      # Select input mode
    --timesteps N \                 # Total training steps
    --render-every M \              # Demo every M episodes
    --frame-skip K \                # Decision every K frames
    --seed S \                      # Random seed
    --device {cuda|cpu}             # Training device
```

### Training Mode Comparison

| Mode | Input Dimension | Speed | Difficulty | Recommended Scene |
|------|----------------|-------|------------|-------------------|
| **Features** | 8-dim vector | ~500 it/s | Easy | Quick validation |
| **Pixels** | 84x84x4 image | ~100 it/s | Hard | End-to-end learning |

**Features Mode Input:**
- Bird Y position
- Bird vertical velocity
- Distance to next pipe (X)
- Next upper pipe Y position
- Next lower pipe Y position
- Distance to following pipe (X)
- Following upper pipe Y position
- Following lower pipe Y position

### Recommended Training Schemes

**Scheme 1: Quick Validation (5 minutes)**
```bash
python train.py --algo ppo --mode features --timesteps 50000 --render-every 5
```
Expected score: 5-10 pipes

**Scheme 2: Stable Training (30 minutes)**
```bash
python train.py --algo ppo --mode features --timesteps 200000 --render-every 20
```
Expected score: 30-50 pipes

**Scheme 3: High Score Challenge (2-3 hours)**
```bash
python train.py --algo ppo --mode features --timesteps 1000000 --render-every 100
```
Expected score: 100+ pipes (can reach 240+)

**Scheme 4: Pixels Mode Training (slower)**
```bash
# DQN Pixels
python train.py --algo dqn --mode pixels --timesteps 200000 --render-every 20 --frame-skip 1

# PPO Pixels
python train.py --algo ppo --mode pixels --timesteps 200000 --render-every 20
```

### Resume from Checkpoint

```bash
# Continue training
python train.py --algo ppo --mode features --timesteps 500000 \
    --resume checkpoints/PPO_features/model_latest.pt
```

---

## Checkpoint Management

### Save Location

Checkpoints are automatically saved in `./checkpoints/` directory:

```
checkpoints/
â”œâ”€â”€ DQN_pixels/
â”‚   â”œâ”€â”€ model_best.pt        # Best model (by score)
â”‚   â”œâ”€â”€ model_latest.pt      # Latest model
â”‚   â””â”€â”€ ...
â”œâ”€â”€ PPO_features/
â”‚   â”œâ”€â”€ model_latest.pt
â”‚   â”œâ”€â”€ model_ep20.pt
â”‚   â””â”€â”€ ...
```

### Save Strategy

| Algorithm | Saved Content | Naming |
|-----------|---------------|--------|
| **DQN** | policy_net, target_net, optimizer, epsilon | model_best.pt, model_ep{N}.pt |
| **PPO** | network (Actor+Critic), optimizer | model_latest.pt, model_ep{N}.pt |

**Note:**
- DQN has model_best.pt (saves highest score model)
- PPO only has model_latest.pt and episode-based models

---

## Model Testing & Demo

### Play Trained Model

```bash
# Basic usage
python play.py --checkpoint <path> --algo <algorithm> --mode <mode>

# Play PPO features latest model
python play.py \
    --checkpoint checkpoints/PPO_features/model_latest.pt \
    --algo ppo \
    --mode features
```

### Common Play Commands

**Play PPO features latest model, 5 episodes:**
```bash
python play.py --checkpoint checkpoints/PPO_features/model_latest.pt --algo ppo --mode features
```

**Play DQN features best model, 10 episodes, slow speed:**
```bash
python play.py --checkpoint checkpoints/DQN_features/model_best.pt --algo dqn --mode features --episodes 10 --fps 30
```

### Visualization Information

During playback, the following is displayed:
- **SCORE**: Current score
- **REWARD**: Episode cumulative reward
- **STEPS**: Flight steps
- **ACTION**: Current action (FLAP/NONE)
- **DIST**: Distance to pipe center

---

## Training Tips

### Reward Function Design (Key!)

Current reward composition (envs/wrappers.py):

```
+25.0   # Pass one pipe (core reward)
+2.0    # Position reward (Gaussian, higher near pipe center)
+0.5    # Extra reward within pipe gap range
+0.05   # Per-frame survival reward
+0.1    # Progress reward (pipe approaching)
-0.2    # Too close to ceiling/floor penalty
-5.0    # Death penalty
```

**Tuning Suggestions:**
- If agent "survives" but doesn't pass pipes: Increase PASS_REWARD (e.g., +50)
- If agent hits ceiling/floor: Increase position reward, strengthen height penalty
- If agent passes but score is low: Add speed reward to encourage quick passing

### TensorBoard Monitoring

```bash
# Start TensorBoard
tensorboard --logdir logs/

# Open browser http://localhost:6006
```

**Key Metrics:**
- episode/score: Score per episode (most important)
- episode/reward: Reward per episode
- step/loss: Training loss
- demo/score: Demo score

---

## Troubleshooting

### Issue 1: CUDA OOM

**Solution:**
```bash
python train.py --algo dqn --mode features --timesteps 100000 --device cpu
```

### Issue 2: ModuleNotFoundError

```bash
conda activate flappy_rl
pip install gymnasium flappy-bird-gymnasium
```

### Issue 3: Score Stuck at 1-2

**Solutions:**
1. Try PPO algorithm (usually converges easier than DQN)
2. Use features mode
3. Extend training time (at least 50k steps)

---

**Happy Training! Looking forward to your agent breaking 100 points!** ğŸš€ğŸ¦

---

## å›¾ç‰‡è¯´æ˜ / Image Notes

**ä¸­æ–‡ï¼š** è¯·å°†æˆªå›¾ä¿å­˜åˆ°ä»¥ä¸‹ä½ç½®ï¼š
- `docs/training_demo.png` - Pygameè®­ç»ƒç•Œé¢æˆªå›¾
- `docs/ppo_1m_result.png` - PPO 100ä¸‡æ­¥è®­ç»ƒç»“æœæˆªå›¾

**English:** Please save screenshots to:
- `docs/training_demo.png` - Pygame training interface screenshot
- `docs/ppo_1m_result.png` - PPO 1M steps training result screenshot
